//! Candle tensor operations backend for optimized neural network operations
//!
//! This module provides optimized implementations of attention, normalization,
//! and other neural network operations using Candle's tensor primitives.

use candle_core::{Device, Tensor};
use candle_nn::ops;
use wasm_chord_core::error::{Error, Result};

/// Candle tensor backend for neural network operations
pub struct CandleTensorBackend {
    device: Device,
}

impl CandleTensorBackend {
    /// Create a new Candle tensor backend
    pub fn new() -> Self {
        Self { device: Device::Cpu }
    }

    /// RMS Normalization using Candle tensors
    ///
    /// # Arguments
    /// * `input` - Input tensor \[seq_len, hidden_size\]
    /// * `weight` - Normalization weights \[hidden_size\]
    /// * `eps` - Epsilon for numerical stability
    ///
    /// # Returns
    /// Normalized tensor \[seq_len, hidden_size\]
    pub fn rms_norm(
        &self,
        input: &[f32],
        weight: &[f32],
        eps: f32,
        seq_len: usize,
        hidden_size: usize,
    ) -> Result<Vec<f32>> {
        // Create tensors
        let input_tensor = Tensor::from_slice(input, (seq_len, hidden_size), &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create input tensor: {}", e)))?;

        let weight_tensor = Tensor::from_slice(weight, hidden_size, &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create weight tensor: {}", e)))?;

        // Apply RMS normalization
        let normalized = ops::rms_norm(&input_tensor, &weight_tensor, eps)
            .map_err(|e| Error::BackendError(format!("RMS norm failed: {}", e)))?;

        // Extract result
        let result = normalized
            .flatten_all()
            .map_err(|e| Error::BackendError(format!("Flatten failed: {}", e)))?
            .to_vec1::<f32>()
            .map_err(|e| Error::BackendError(format!("Failed to extract result: {}", e)))?;

        Ok(result)
    }

    /// Scaled Dot-Product Attention using Candle tensors
    ///
    /// # Arguments
    /// * `q` - Query tensor \[seq_len, num_heads, head_dim\]
    /// * `k` - Key tensor \[seq_len, num_heads, head_dim\]
    /// * `v` - Value tensor \[seq_len, num_heads, head_dim\]
    /// * `scale` - Attention scale factor (usually 1/sqrt(head_dim))
    ///
    /// # Returns
    /// Attention output \[seq_len, num_heads, head_dim\]
    #[allow(clippy::too_many_arguments)]
    pub fn scaled_dot_product_attention(
        &self,
        q: &[f32],
        k: &[f32],
        v: &[f32],
        scale: f32,
        seq_len: usize,
        num_heads: usize,
        head_dim: usize,
    ) -> Result<Vec<f32>> {
        // Reshape inputs to [seq_len, num_heads, head_dim]
        let q_tensor = Tensor::from_slice(q, (seq_len, num_heads, head_dim), &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create Q tensor: {}", e)))?;
        let k_tensor = Tensor::from_slice(k, (seq_len, num_heads, head_dim), &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create K tensor: {}", e)))?;
        let v_tensor = Tensor::from_slice(v, (seq_len, num_heads, head_dim), &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create V tensor: {}", e)))?;

        // Transpose to [num_heads, seq_len, head_dim] for attention computation
        let q_t = q_tensor
            .transpose(0, 1)
            .map_err(|e| Error::BackendError(format!("Q transpose failed: {}", e)))?;
        let k_t = k_tensor
            .transpose(0, 1)
            .map_err(|e| Error::BackendError(format!("K transpose failed: {}", e)))?;
        let v_t = v_tensor
            .transpose(0, 1)
            .map_err(|e| Error::BackendError(format!("V transpose failed: {}", e)))?;

        // Compute attention scores: Q @ K^T
        let scores = q_t
            .matmul(
                &k_t.transpose(2, 3)
                    .map_err(|e| Error::BackendError(format!("K transpose failed: {}", e)))?,
            )
            .map_err(|e| {
                Error::BackendError(format!("Attention scores computation failed: {}", e))
            })?;

        // Scale the scores
        let scale_tensor = Tensor::new(&[scale], &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create scale tensor: {}", e)))?;
        let scaled_scores = (&scores * &scale_tensor)
            .map_err(|e| Error::BackendError(format!("Scaling failed: {}", e)))?;

        // Apply softmax
        let attention_weights = ops::softmax(&scaled_scores, 3)
            .map_err(|e| Error::BackendError(format!("Softmax failed: {}", e)))?;

        // Apply attention to values: attention_weights @ V
        let attended = attention_weights
            .matmul(&v_t)
            .map_err(|e| Error::BackendError(format!("Attention application failed: {}", e)))?;

        // Transpose back to [seq_len, num_heads, head_dim]
        let result = attended
            .transpose(0, 1)
            .map_err(|e| Error::BackendError(format!("Result transpose failed: {}", e)))?;

        // Flatten and return
        let result_vec = result
            .flatten_all()
            .map_err(|e| Error::BackendError(format!("Flatten failed: {}", e)))?
            .to_vec1::<f32>()
            .map_err(|e| Error::BackendError(format!("Failed to extract result: {}", e)))?;

        Ok(result_vec)
    }

    /// SiLU (Swish) activation function using Candle tensors
    ///
    /// # Arguments
    /// * `input` - Input tensor
    ///
    /// # Returns
    /// SiLU-activated tensor
    pub fn silu(&self, input: &[f32], len: usize) -> Result<Vec<f32>> {
        let input_tensor = Tensor::from_slice(input, len, &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create input tensor: {}", e)))?;

        let silu_output = ops::silu(&input_tensor)
            .map_err(|e| Error::BackendError(format!("SiLU failed: {}", e)))?;

        let result = silu_output
            .to_vec1::<f32>()
            .map_err(|e| Error::BackendError(format!("Failed to extract result: {}", e)))?;

        Ok(result)
    }

    /// Softmax operation using Candle tensors
    ///
    /// # Arguments
    /// * `logits` - Input logits \[vocab_size\]
    /// * `dim_size` - Dimension size for softmax
    ///
    /// # Returns
    /// Softmax probabilities \[vocab_size\]
    pub fn softmax(&self, logits: &[f32], dim_size: usize) -> Result<Vec<f32>> {
        let logits_tensor = Tensor::from_slice(logits, dim_size, &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create logits tensor: {}", e)))?;

        let softmax_output = ops::softmax(&logits_tensor, 0)
            .map_err(|e| Error::BackendError(format!("Softmax failed: {}", e)))?;

        let result = softmax_output
            .to_vec1::<f32>()
            .map_err(|e| Error::BackendError(format!("Failed to extract result: {}", e)))?;

        Ok(result)
    }

    /// Rotary Position Embedding (RoPE) using Candle tensors
    ///
    /// # Arguments
    /// * `tensor` - Input tensor \[seq_len, num_heads, head_dim\]
    /// * `freqs_cos` - Cosine frequencies \[seq_len, head_dim\]
    /// * `freqs_sin` - Sine frequencies \[seq_len, head_dim\]
    /// * `position_offset` - Position offset for KV cache
    ///
    /// # Returns
    /// RoPE-applied tensor \[seq_len, num_heads, head_dim\]
    #[allow(clippy::too_many_arguments)]
    pub fn rope(
        &self,
        tensor: &[f32],
        freqs_cos: &[f32],
        freqs_sin: &[f32],
        seq_len: usize,
        num_heads: usize,
        head_dim: usize,
        _position_offset: usize,
    ) -> Result<Vec<f32>> {
        // For now, implement a simplified RoPE that works with Candle
        // This applies rotation to pairs of dimensions

        let mut result = tensor.to_vec();

        // Apply rotation to pairs of dimensions (0,1), (2,3), etc.
        for seq in 0..seq_len {
            for head in 0..num_heads {
                for i in (0..head_dim).step_by(2) {
                    if i + 1 < head_dim {
                        let base_idx = seq * num_heads * head_dim + head * head_dim;
                        let x_idx = base_idx + i;
                        let y_idx = base_idx + i + 1;

                        let cos_idx = seq * head_dim + i;
                        let sin_idx = seq * head_dim + i;

                        if x_idx < result.len()
                            && y_idx < result.len()
                            && cos_idx < freqs_cos.len()
                            && sin_idx < freqs_sin.len()
                        {
                            let x = result[x_idx];
                            let y = result[y_idx];
                            let cos_val = freqs_cos[cos_idx];
                            let sin_val = freqs_sin[sin_idx];

                            // Apply rotation: x' = x*cos - y*sin, y' = x*sin + y*cos
                            result[x_idx] = x * cos_val - y * sin_val;
                            result[y_idx] = x * sin_val + y * cos_val;
                        }
                    }
                }
            }
        }

        Ok(result)
    }

    /// Element-wise addition using Candle tensors
    pub fn add(&self, a: &[f32], b: &[f32], len: usize) -> Result<Vec<f32>> {
        let a_tensor = Tensor::from_slice(a, len, &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create tensor A: {}", e)))?;

        let b_tensor = Tensor::from_slice(b, len, &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create tensor B: {}", e)))?;

        let sum = (&a_tensor + &b_tensor)
            .map_err(|e| Error::BackendError(format!("Addition failed: {}", e)))?;

        let result = sum
            .to_vec1::<f32>()
            .map_err(|e| Error::BackendError(format!("Failed to extract result: {}", e)))?;

        Ok(result)
    }

    /// Element-wise multiplication using Candle tensors
    pub fn mul(&self, a: &[f32], b: &[f32], len: usize) -> Result<Vec<f32>> {
        let a_tensor = Tensor::from_slice(a, len, &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create tensor A: {}", e)))?;

        let b_tensor = Tensor::from_slice(b, len, &self.device)
            .map_err(|e| Error::BackendError(format!("Failed to create tensor B: {}", e)))?;

        let product = (&a_tensor * &b_tensor)
            .map_err(|e| Error::BackendError(format!("Multiplication failed: {}", e)))?;

        let result = product
            .to_vec1::<f32>()
            .map_err(|e| Error::BackendError(format!("Failed to extract result: {}", e)))?;

        Ok(result)
    }

    /// Get device information
    pub fn device_info(&self) -> &'static str {
        match self.device {
            Device::Cpu => "CPU",
            Device::Cuda(_) => "CUDA",
            Device::Metal(_) => "Metal",
        }
    }
}

impl Default for CandleTensorBackend {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_rms_norm() {
        let backend = CandleTensorBackend::new();
        let input = vec![1.0, 2.0, 3.0, 4.0];
        let weight = vec![1.0, 1.0, 1.0, 1.0];

        let result = backend.rms_norm(&input, &weight, 1e-6, 1, 4).unwrap();

        // RMS norm should normalize the input
        assert_eq!(result.len(), 4);
        assert!(result[0].abs() < 1.0);
    }

    #[test]
    fn test_silu() {
        let backend = CandleTensorBackend::new();
        let input = vec![0.0, 1.0, -1.0, 2.0];

        let result = backend.silu(&input, 4).unwrap();

        assert_eq!(result.len(), 4);
        // SiLU(0) = 0, SiLU(1) ≈ 0.73, SiLU(-1) ≈ -0.27, SiLU(2) ≈ 1.76
        assert!((result[0] - 0.0).abs() < 1e-6);
        assert!((result[1] - 0.7310586).abs() < 1e-5);
    }

    #[test]
    fn test_softmax() {
        let backend = CandleTensorBackend::new();
        let logits = vec![1.0, 2.0, 3.0];

        let result = backend.softmax(&logits, 3).unwrap();

        assert_eq!(result.len(), 3);
        // Softmax should sum to 1.0
        let sum: f32 = result.iter().sum();
        assert!((sum - 1.0).abs() < 1e-6);
        // Largest value should have highest probability
        assert!(result[2] > result[1]);
        assert!(result[1] > result[0]);
    }

    #[test]
    fn test_add() {
        let backend = CandleTensorBackend::new();
        let a = vec![1.0, 2.0, 3.0];
        let b = vec![4.0, 5.0, 6.0];

        let result = backend.add(&a, &b, 3).unwrap();

        assert_eq!(result, vec![5.0, 7.0, 9.0]);
    }

    #[test]
    fn test_mul() {
        let backend = CandleTensorBackend::new();
        let a = vec![2.0, 3.0, 4.0];
        let b = vec![5.0, 6.0, 7.0];

        let result = backend.mul(&a, &b, 3).unwrap();

        assert_eq!(result, vec![10.0, 18.0, 28.0]);
    }
}
