//! Production-Hardened Memory64 Runtime
//!
//! This is the PRODUCTION-GRADE implementation with critical fixes:
//! 1. ✅ parking_lot::Mutex (no poisoning, better performance)
//! 2. ✅ Integer overflow checks on all arithmetic
//! 3. ✅ WASM pointer validation in host functions
//! 4. ✅ Proper error logging
//!
//! Battle-tested patterns from:
//! - wasmex (Elixir/Erlang WASM runtime) - fine-grained locking
//! - Wasmtime/Wasmer - pointer validation, error handling
//! - llama.cpp - layer loading patterns

use anyhow::{anyhow, Context, Result};
use parking_lot::Mutex; // No poisoning, faster than std::sync::Mutex
use std::sync::Arc;
use wasmtime::{AsContext, Caller, Extern, Linker, Memory, MemoryType, Store};

/// Memory access statistics for monitoring (thread-safe)
#[derive(Debug, Clone, Default)]
pub struct MemoryStats {
    pub reads: u64,
    pub writes: u64,
    pub bytes_read: u64,
    pub bytes_written: u64,
    pub errors: u64, // Track error count
}

/// Memory region descriptor for multi-memory layouts
#[derive(Debug, Clone)]
pub struct MemoryRegion {
    pub id: u32,
    pub name: String,
    pub start_offset: u64,
    pub size: u64,
    pub purpose: String,
}

impl MemoryRegion {
    pub fn new(
        id: u32,
        name: impl Into<String>,
        start_offset: u64,
        size: u64,
        purpose: impl Into<String>,
    ) -> Result<Self> {
        // ✅ FIX 2: Validate size is non-zero and page-aligned
        if size == 0 {
            return Err(anyhow!("Region size cannot be zero"));
        }
        if !size.is_multiple_of(65536) {
            return Err(anyhow!(
                "Region size must be page-aligned (multiple of 64KB), got {}",
                size
            ));
        }

        // ✅ FIX 2: Check for offset overflow
        let _end = start_offset
            .checked_add(size)
            .ok_or_else(|| anyhow!("Region offset + size overflows u64"))?;

        Ok(Self { id, name: name.into(), start_offset, size, purpose: purpose.into() })
    }

    /// Check if an offset falls within this region
    pub fn contains(&self, offset: u64) -> bool {
        // ✅ FIX 2: Use checked arithmetic
        if let Some(end) = self.start_offset.checked_add(self.size) {
            offset >= self.start_offset && offset < end
        } else {
            false // Overflow means invalid region
        }
    }

    /// Get local offset within this region
    pub fn local_offset(&self, global_offset: u64) -> Result<u64> {
        if !self.contains(global_offset) {
            return Err(anyhow!(
                "Offset {} not in region {} ({}..{})",
                global_offset,
                self.name,
                self.start_offset,
                self.start_offset.saturating_add(self.size)
            ));
        }
        Ok(global_offset - self.start_offset)
    }
}

/// Memory layout configuration
#[derive(Debug, Clone)]
pub struct MemoryLayout {
    pub regions: Vec<MemoryRegion>,
    pub total_size: u64,
}

impl MemoryLayout {
    /// Create a single memory layout with validation
    pub fn single(size_gb: u64, purpose: impl Into<String>) -> Result<Self> {
        // ✅ FIX 2: Validate input
        if size_gb == 0 {
            return Err(anyhow!("Size must be greater than 0"));
        }
        if size_gb > 16384 {
            // 16TB limit (reasonable for now)
            return Err(anyhow!("Size {} GB exceeds maximum 16TB", size_gb));
        }

        // ✅ FIX 2: Check for overflow when converting to bytes
        let size = size_gb
            .checked_mul(1024)
            .and_then(|v| v.checked_mul(1024))
            .and_then(|v| v.checked_mul(1024))
            .ok_or_else(|| anyhow!("Size {} GB causes overflow", size_gb))?;

        let region = MemoryRegion::new(0, "memory0", 0, size, purpose)?;

        Ok(Self { regions: vec![region], total_size: size })
    }

    /// Create a multi-memory layout with custom regions
    pub fn multi(regions: &[(&str, u64)]) -> Result<Self> {
        if regions.is_empty() {
            return Err(anyhow!("Must provide at least one region"));
        }

        let mut offset = 0u64;
        let mut region_list = Vec::new();

        for (id, (name, size_gb)) in regions.iter().enumerate() {
            // ✅ FIX 2: Validate and check overflow
            if *size_gb == 0 {
                return Err(anyhow!("Region '{}' has zero size", name));
            }

            let size = size_gb
                .checked_mul(1024)
                .and_then(|v| v.checked_mul(1024))
                .and_then(|v| v.checked_mul(1024))
                .ok_or_else(|| anyhow!("Size {} GB causes overflow", size_gb))?;

            let region =
                MemoryRegion::new(id as u32, format!("memory{}", id), offset, size, *name)?;

            // ✅ FIX 2: Check offset overflow
            offset = offset
                .checked_add(size)
                .ok_or_else(|| anyhow!("Total memory layout size overflows u64"))?;

            region_list.push(region);
        }

        Ok(Self { regions: region_list, total_size: offset })
    }

    /// Find the region containing the given offset
    pub fn find_region(&self, offset: u64) -> Result<&MemoryRegion> {
        self.regions.iter().find(|r| r.contains(offset)).ok_or_else(|| {
            anyhow!(
                "Offset {} not in any memory region (total size: {} GB)",
                offset,
                self.total_size / 1024 / 1024 / 1024
            )
        })
    }

    /// Get total memory in GB
    pub fn total_gb(&self) -> f64 {
        self.total_size as f64 / 1024.0 / 1024.0 / 1024.0
    }
}

/// Layer information for model weights tracking
#[derive(Debug, Clone)]
pub struct LayerInfo {
    pub layer_id: u32,
    pub offset: u64,
    pub size: usize,
    pub memory_id: u32,
    pub layer_type: String,
}

/// Production Memory64 state with thread-safe access
pub struct Memory64State {
    /// Memory64 instances (one per region)
    memories: Vec<Memory>,
    /// Memory layout configuration
    layout: MemoryLayout,
    /// Layer tracking
    layers: Vec<LayerInfo>,
    /// Statistics
    stats: MemoryStats,
    /// Feature flag
    enabled: bool,
}

impl Memory64State {
    /// Create new Memory64 state
    pub fn new(layout: MemoryLayout, enabled: bool) -> Self {
        Self {
            memories: Vec::new(),
            layout,
            layers: Vec::new(),
            stats: MemoryStats::default(),
            enabled,
        }
    }

    /// Initialize Memory64 instances with validation
    pub fn initialize(&mut self, store: &mut Store<()>) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }

        for region in &self.layout.regions {
            // ✅ FIX 2: Validate before division
            if region.size == 0 {
                return Err(anyhow!("Region '{}' has zero size", region.name));
            }

            // Calculate pages (64KB per page)
            let min_pages = region.size / 65536;
            let max_pages = min_pages.checked_mul(2); // Allow 2x growth

            let memory_type = MemoryType::new64(min_pages, max_pages);
            let memory = Memory::new(&mut *store, memory_type)
                .with_context(|| format!("Failed to create Memory64 for region {}", region.name))?;

            self.memories.push(memory);

            // ✅ FIX 4: Use log crate instead of println
            #[cfg(feature = "log")]
            log::info!(
                "Memory64 region '{}' ({}): {:.2}GB",
                region.name,
                region.purpose,
                region.size as f64 / 1024.0 / 1024.0 / 1024.0
            );
        }

        Ok(())
    }

    /// Write data to Memory64 with bounds checking
    pub fn write(&mut self, store: &mut Store<()>, offset: u64, data: &[u8]) -> Result<()> {
        if !self.enabled {
            return Err(anyhow!("Memory64 not enabled"));
        }

        let region = self.layout.find_region(offset)?;
        let local_offset = region.local_offset(offset)?;
        let memory = &self.memories[region.id as usize];

        // ✅ FIX 2: Check for overflow
        let end_offset = local_offset
            .checked_add(data.len() as u64)
            .ok_or_else(|| anyhow!("Write offset + size overflows u64"))?;

        if end_offset > region.size {
            return Err(anyhow!(
                "Write would exceed region {} bounds (offset: {}, size: {}, region size: {})",
                region.name,
                local_offset,
                data.len(),
                region.size
            ));
        }

        memory
            .write(store, local_offset as usize, data)
            .with_context(|| format!("Failed to write to region {}", region.name))?;

        // Update stats
        self.stats.writes += 1;
        self.stats.bytes_written += data.len() as u64;

        Ok(())
    }

    /// Read data from Memory64 with bounds checking
    pub fn read<T>(
        &mut self,
        store: impl AsContext<Data = T>,
        offset: u64,
        buffer: &mut [u8],
    ) -> Result<()> {
        if !self.enabled {
            return Err(anyhow!("Memory64 not enabled"));
        }

        let region = self.layout.find_region(offset)?;
        let local_offset = region.local_offset(offset)?;
        let memory = &self.memories[region.id as usize];

        // ✅ FIX 2: Check for overflow
        let end_offset = local_offset
            .checked_add(buffer.len() as u64)
            .ok_or_else(|| anyhow!("Read offset + size overflows u64"))?;

        if end_offset > region.size {
            return Err(anyhow!(
                "Read would exceed region {} bounds (offset: {}, size: {}, region size: {})",
                region.name,
                local_offset,
                buffer.len(),
                region.size
            ));
        }

        memory
            .read(&store, local_offset as usize, buffer)
            .with_context(|| format!("Failed to read from region {}", region.name))?;

        // Update stats
        self.stats.reads += 1;
        self.stats.bytes_read += buffer.len() as u64;

        Ok(())
    }

    /// Register a layer's location
    pub fn register_layer(
        &mut self,
        layer_id: u32,
        offset: u64,
        size: usize,
        layer_type: impl Into<String>,
    ) -> Result<()> {
        let region = self.layout.find_region(offset)?;

        self.layers.push(LayerInfo {
            layer_id,
            offset,
            size,
            memory_id: region.id,
            layer_type: layer_type.into(),
        });

        Ok(())
    }

    /// Get layer information
    pub fn get_layer(&self, layer_id: u32) -> Result<&LayerInfo> {
        self.layers
            .iter()
            .find(|l| l.layer_id == layer_id)
            .ok_or_else(|| anyhow!("Layer {} not found", layer_id))
    }

    /// Get statistics
    pub fn stats(&self) -> &MemoryStats {
        &self.stats
    }

    /// Get layout information
    pub fn layout(&self) -> &MemoryLayout {
        &self.layout
    }
}

/// Production-ready Memory64 runtime with parking_lot::Mutex
pub struct Memory64Runtime {
    state: Arc<Mutex<Memory64State>>, // ✅ FIX 1: parking_lot::Mutex (no poisoning)
}

impl Memory64Runtime {
    /// Create new runtime
    pub fn new(layout: MemoryLayout, enabled: bool) -> Self {
        Self { state: Arc::new(Mutex::new(Memory64State::new(layout, enabled))) }
    }

    /// Initialize Memory64 instances
    pub fn initialize(&self, store: &mut Store<()>) -> Result<()> {
        self.state.lock().initialize(store) // ✅ FIX 1: No poison handling needed
    }

    /// Write model data
    pub fn write_model_data(&self, store: &mut Store<()>, offset: u64, data: &[u8]) -> Result<()> {
        self.state.lock().write(store, offset, data)
    }

    /// Register a layer
    pub fn register_layer(
        &self,
        _store: &mut Store<()>,
        layer_id: u32,
        layer_type: impl Into<String>,
        offset: u64,
        size: usize,
    ) -> Result<()> {
        self.state.lock().register_layer(layer_id, offset, size, layer_type)
    }

    /// Get statistics
    pub fn stats(&self) -> Result<MemoryStats> {
        Ok(self.state.lock().stats().clone())
    }

    /// Get statistics (convenience method for host example)
    pub fn get_stats<T>(&self, _store: &Store<T>) -> Result<MemoryStats> {
        Ok(self.state.lock().stats().clone())
    }

    /// Add host functions to linker with WASM pointer validation
    pub fn add_to_linker(&self, linker: &mut Linker<()>) -> Result<()> {
        let state = self.state.clone();

        // Host function: Load layer weights from Memory64 to WASM memory
        linker.func_wrap(
            "env",
            "memory64_load_layer",
            move |mut caller: Caller<'_, ()>, layer_id: u32, wasm_ptr: u32, max_size: u32| -> i32 {
                let state_clone = state.clone();
                let mut state_guard = state_clone.lock(); // ✅ FIX 1: No poison check

                if !state_guard.enabled {
                    return -1;
                }

                // Get layer info
                let layer = match state_guard.get_layer(layer_id) {
                    Ok(l) => l.clone(),
                    Err(e) => {
                        eprintln!("❌ Layer {} not found: {}", layer_id, e); // ✅ FIX 4: Log error
                        return -2;
                    }
                };

                if layer.size > max_size as usize {
                    eprintln!(
                        "❌ Buffer too small for layer {}: need {}, got {}",
                        layer_id, layer.size, max_size
                    );
                    return -3;
                }

                // ✅ FIX 3: Validate WASM pointer BEFORE allocating buffer
                let wasm_memory = match caller.get_export("memory") {
                    Some(Extern::Memory(mem)) => mem,
                    _ => {
                        eprintln!("❌ No WASM memory export available");
                        return -5;
                    }
                };

                let wasm_mem_size = wasm_memory.data_size(&caller);

                // ✅ FIX 3: Check pointer bounds with overflow protection
                let end_ptr = match (wasm_ptr as usize).checked_add(layer.size) {
                    Some(end) => end,
                    None => {
                        eprintln!("❌ WASM pointer overflow: {} + {}", wasm_ptr, layer.size);
                        state_guard.stats.errors += 1;
                        return -6;
                    }
                };

                if end_ptr > wasm_mem_size {
                    eprintln!(
                        "❌ WASM pointer out of bounds: {} + {} > {}",
                        wasm_ptr, layer.size, wasm_mem_size
                    );
                    state_guard.stats.errors += 1;
                    return -7;
                }

                // Read from Memory64
                let mut buffer = vec![0u8; layer.size];
                if let Err(e) = state_guard.read(caller.as_context(), layer.offset, &mut buffer) {
                    eprintln!("❌ Failed to read layer {}: {}", layer_id, e); // ✅ FIX 4: Log error
                    state_guard.stats.errors += 1;
                    return -4;
                }

                // Write to WASM memory (already validated)
                if let Err(e) = wasm_memory.write(&mut caller, wasm_ptr as usize, &buffer) {
                    eprintln!("❌ Failed to write to WASM memory: {}", e);
                    state_guard.stats.errors += 1;
                    return -8;
                }

                layer.size as i32
            },
        )?;

        // Host function: Read data from Memory64
        let state2 = self.state.clone();
        linker.func_wrap(
            "env",
            "memory64_read",
            move |mut caller: Caller<'_, ()>, offset: u64, wasm_ptr: u32, size: u32| -> i32 {
                let state_clone = state2.clone();
                let mut state_guard = state_clone.lock();

                if !state_guard.enabled {
                    return -1;
                }

                // ✅ FIX 3: Validate WASM pointer
                let wasm_memory = match caller.get_export("memory") {
                    Some(Extern::Memory(mem)) => mem,
                    _ => {
                        eprintln!("❌ No WASM memory export");
                        return -3;
                    }
                };

                let wasm_mem_size = wasm_memory.data_size(&caller);

                // ✅ FIX 3: Check overflow
                let end_ptr = match (wasm_ptr as usize).checked_add(size as usize) {
                    Some(end) => end,
                    None => {
                        eprintln!("❌ WASM pointer overflow: {} + {}", wasm_ptr, size);
                        state_guard.stats.errors += 1;
                        return -4;
                    }
                };

                if end_ptr > wasm_mem_size {
                    eprintln!(
                        "❌ WASM pointer out of bounds: {} + {} > {}",
                        wasm_ptr, size, wasm_mem_size
                    );
                    state_guard.stats.errors += 1;
                    return -5;
                }

                // Read from Memory64
                let mut buffer = vec![0u8; size as usize];
                if let Err(e) = state_guard.read(caller.as_context(), offset, &mut buffer) {
                    eprintln!("❌ memory64_read failed at offset {}: {}", offset, e); // ✅ FIX 4
                    state_guard.stats.errors += 1;
                    return -2;
                }

                // Write to WASM memory
                if let Err(e) = wasm_memory.write(&mut caller, wasm_ptr as usize, &buffer) {
                    eprintln!("❌ Failed to write to WASM memory: {}", e);
                    state_guard.stats.errors += 1;
                    return -6;
                }

                size as i32
            },
        )?;

        // Host function: Check if Memory64 is enabled
        let state3 = self.state.clone();
        linker.func_wrap("env", "memory64_is_enabled", move |_caller: Caller<'_, ()>| -> i32 {
            if state3.lock().enabled {
                1
            } else {
                0
            }
        })?;

        // Host function: Get memory stats
        let state4 = self.state.clone();
        linker.func_wrap("env", "memory64_stats", move |_caller: Caller<'_, ()>| -> i64 {
            state4.lock().stats().reads as i64
        })?;

        Ok(())
    }

    /// Get state reference (for testing/debugging)
    pub fn state(&self) -> Arc<Mutex<Memory64State>> {
        self.state.clone()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_single_memory_layout() {
        let layout = MemoryLayout::single(8, "test").unwrap();
        assert_eq!(layout.regions.len(), 1);
        assert_eq!(layout.total_gb(), 8.0);
    }

    #[test]
    fn test_invalid_size() {
        assert!(MemoryLayout::single(0, "test").is_err()); // Zero size
        assert!(MemoryLayout::single(20000, "test").is_err()); // Too large
    }

    #[test]
    fn test_overflow_protection() {
        // This should fail due to overflow when converting GB to bytes
        // u64::MAX / (1024^3) = ~16 exabytes, when multiplied by 1024^3 should overflow
        // Use a value that will overflow: (u64::MAX / 1024 / 1024 / 1024) + 1
        let result = MemoryLayout::multi(&[
            ("region1", u64::MAX / 1024 / 1024 / 1024),
            ("region2", 1), // This will cause offset overflow
        ]);
        assert!(result.is_err());
    }

    #[test]
    fn test_region_validation() {
        // Non-page-aligned size should fail
        let result = MemoryRegion::new(0, "test", 0, 1000, "test");
        assert!(result.is_err());

        // Valid page-aligned size should succeed
        let result = MemoryRegion::new(0, "test", 0, 65536, "test");
        assert!(result.is_ok());
    }
}
